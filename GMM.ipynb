{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgWtqPbmU5X0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/235kaggle'"
      ],
      "metadata": {
        "id": "Bc2G7nRHVEfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime, date\n",
        "import warnings\n",
        "import os\n",
        "from functools import reduce\n",
        "import re"
      ],
      "metadata": {
        "id": "UJRlddp7VXXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(file_path)"
      ],
      "metadata": {
        "id": "4bqGm-m1YNyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opening the datasets"
      ],
      "metadata": {
        "id": "CcHRJ4bGd64R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data files\n",
        "logon = pd.read_csv(f'{file_path}/logon.csv')\n",
        "device = pd.read_csv(f'{file_path}/device.csv')\n",
        "email = pd.read_csv(f'{file_path}/email.csv')\n",
        "file_data = pd.read_csv(f'{file_path}/file.csv')\n",
        "psychometric = pd.read_csv(f'{file_path}/psychometric.csv')\n",
        "decoys = pd.read_csv(f'{file_path}/decoy_file.csv')\n",
        "\n",
        "print(\"Shapes of the files:\")\n",
        "print(f\"logon: {logon.shape}\")\n",
        "print(f\"device: {device.shape}\")\n",
        "print(f\"email: {email.shape}\")\n",
        "print(f\"file_data: {file_data.shape}\")\n",
        "print(f\"psychometric: {psychometric.shape}\")\n",
        "print(f\"decoys: {decoys.shape}\")"
      ],
      "metadata": {
        "id": "13TKSF0mWaFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    'Logon': logon,\n",
        "    'Device': device,\n",
        "    'Email': email,\n",
        "    'File': file_data,\n",
        "    'Psychometric': psychometric,\n",
        "    'Decoys': decoys\n",
        "}\n",
        "\n",
        "# Viewing the structure of the datasets\n",
        "for name, df in datasets.items():\n",
        "    print(f\"======{name} Dataset=====\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"\\nColumns and Data Types:\")\n",
        "    display(df.dtypes)\n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    display(df.head(5))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "bDqUnnrpWsIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "qJhvFrC33zqA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "995a14b0"
      },
      "source": [
        "def process_datetime_columns(df, date_column):\n",
        "    df[date_column] = pd.to_datetime(df[date_column], format='%m/%d/%Y %H:%M:%S')\n",
        "    df['hour'] = df[date_column].dt.hour\n",
        "    df['day_of_week'] = df[date_column].dt.day_name()\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97d358ed"
      },
      "source": [
        "email = process_datetime_columns(email, 'date')\n",
        "file_data = process_datetime_columns(file_data, 'date')\n",
        "\n",
        "print(\"\\nEmail DataFrame after processing:\")\n",
        "display(email.head(5))\n",
        "display(email.dtypes)\n",
        "\n",
        "print(\"\\nFile Data DataFrame after processing:\")\n",
        "display(file_data.head(5))\n",
        "display(file_data.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "def get_time_features_flags(df, date_column):\n",
        "    df[date_column] = pd.to_datetime(df[date_column], format='%m/%d/%Y %H:%M:%S', errors='coerce')\n",
        "    df['hour'] = df[date_column].dt.hour\n",
        "    df['day'] = df[date_column].dt.date\n",
        "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
        "\n",
        "    # After hours: before 7 AM or after 7 PM\n",
        "    df['is_after_hours'] = ((df['hour'] < 7) | (df['hour'] >= 19)).astype(int)\n",
        "    if 'to' in df.columns:\n",
        "        df['is_external'] = df['to'].apply(lambda x: 0 if isinstance(x, str) and 'dtaa.com' in x else 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Counting email recipients\n",
        "def count_recipients(row):\n",
        "    count = 0\n",
        "    for field in ['to', 'cc', 'bcc']:\n",
        "        if pd.notna(row[field]) and row[field]:\n",
        "            count += len(str(row[field]).split(';'))\n",
        "    return count\n",
        "\n",
        "logon = get_time_features_flags(logon.copy(), 'date')\n",
        "device = get_time_features_flags(device.copy(), 'date')\n",
        "email = get_time_features_flags(email.copy(), 'date')\n",
        "file_data = get_time_features_flags(file_data.copy(), 'date')"
      ],
      "metadata": {
        "id": "-odyqsC_Qr5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "cvNK5uLG34UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PC Mapping\n",
        "user_pc_counts = logon.groupby(['user', 'pc']).size().reset_index(name='count')\n",
        "user_primary_pc = user_pc_counts.loc[user_pc_counts.groupby('user')['count'].idxmax()][['user', 'pc']]\n",
        "user_primary_pc = dict(zip(user_primary_pc['user'], user_primary_pc['pc']))\n",
        "\n",
        "logon['is_primary_pc'] = logon.apply(lambda x: x['pc'] == user_primary_pc.get(x['user'], ''), axis=1)\n",
        "print(\"Mapped primary PC for each user.\")\n",
        "\n",
        "# Aggregating Features By User and Day\n",
        "print(\"Aggregating Features by User-Day\")\n",
        "print(\"==============\")\n",
        "\n",
        "# Logon Features\n",
        "print(\"Using logon features...\")\n",
        "logon_agg = logon[logon['activity'] == 'Logon'].groupby(['user', 'day']).agg(\n",
        "    daily_logon_count=('id', 'count'),\n",
        "    daily_after_hours_logons=('is_after_hours', 'sum'),\n",
        "    daily_unique_pcs=('pc', 'nunique')\n",
        ").reset_index()\n",
        "\n",
        "# Foreign PC Logons, for outside logins\n",
        "foreign_logons = logon[(logon['activity'] == 'Logon') & (~logon['is_primary_pc'])].groupby(['user', 'day']).size().reset_index(name='daily_foreign_pc_logons')\n",
        "logon_agg = logon_agg.merge(foreign_logons, on=['user', 'day'], how='left').fillna(0)\n",
        "\n",
        "\n",
        "# File Features\n",
        "print(\"Using file features...\")\n",
        "file_agg = file_data.groupby(['user', 'day']).agg(\n",
        "    daily_file_reads=('activity', lambda x: (x == 'Read').sum()),\n",
        "    daily_file_writes=('activity', lambda x: (x == 'Write').sum()),\n",
        "    daily_file_deletes=('activity', lambda x: (x == 'Delete').sum()),\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "# Device Features\n",
        "print(\"Using device features...\")\n",
        "device_agg = device[device['activity'] == 'Connect'].groupby(['user', 'day']).agg(\n",
        "    daily_device_connects=('id', 'count'),\n",
        "    daily_after_hours_device=('is_after_hours', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "# Email Features\n",
        "email['recipient_count'] = email.apply(count_recipients, axis=1)\n",
        "\n",
        "email_sent = email[email['activity'] == 'Send']\n",
        "email_agg = email_sent.groupby(['user', 'day']).agg(\n",
        "    daily_emails_sent=('id', 'count'),\n",
        "    daily_max_recipients=('recipient_count', 'max'),\n",
        "    daily_total_recipients=('recipient_count', 'sum'),\n",
        "    daily_external_emails=('is_external', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "print(\"Individual log aggregation complete.\")"
      ],
      "metadata": {
        "id": "DDdHuqIVUA8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging Features Into One Dataframe\n",
        "daily_agg_dfs = [logon_agg, file_agg, device_agg, email_agg]\n",
        "\n",
        "features_df = daily_agg_dfs[0].copy()\n",
        "\n",
        "for df in daily_agg_dfs[1:]:\n",
        "    features_df = features_df.merge(df, on=['user', 'day'], how='outer')\n",
        "\n",
        "features_df = features_df.fillna(0)\n",
        "\n",
        "features_df = features_df.set_index(['user', 'day'])\n",
        "\n",
        "print(\"Feature Matrix For GMM\")\n",
        "print(\"---------------\")\n",
        "print(f\"Feature Matrix Shape: {features_df.shape}\")\n",
        "print(f\"\\nList of Features in Matrix:\")\n",
        "print(features_df.columns.tolist())\n",
        "print(f\"\\nSample Data:\")\n",
        "display(features_df.head(5))"
      ],
      "metadata": {
        "id": "AwKxpDnTU5El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principle Component Analysis"
      ],
      "metadata": {
        "id": "TRdKAF7NZ5Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "X_scaled = StandardScaler().fit_transform(features_df)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, index=features_df.index, columns=features_df.columns)\n",
        "\n",
        "# Running PCA for the dimensionality reduction\n",
        "pca = PCA(n_components=0.90) # retaining 90% varience\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "optimal = X_pca.shape[1]\n",
        "\n",
        "pca_df = pd.DataFrame(\n",
        "    data=X_pca,\n",
        "    columns=[f'PC{i+1}' for i in range(optimal)],\n",
        "    index=features_df.index\n",
        ")\n",
        "print(f\"Going from {features_df.shape[1]} features to {optimal} dimensions.\")\n",
        "\n",
        "X_full_pca = pca_df.values"
      ],
      "metadata": {
        "id": "2giYpVZcVYui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GMM Model"
      ],
      "metadata": {
        "id": "5f28ay1yZ8ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Mixture Model (GMM) Anomaly Scoring\n",
        "N_optimal=5 # This was found from the PCA\n",
        "n_components = N_optimal\n",
        "gmm = GaussianMixture(n_components=N_optimal, random_state=42)\n",
        "gmm.fit(X_full_pca)\n",
        "\n",
        "# score_samples returns the log-likelihood (LL).\n",
        "log_likelihoods = gmm.score_samples(X_full_pca)\n",
        "pca_df['GMM_Anomaly_Score'] = -log_likelihoods\n",
        "\n",
        "#Ranking them\n",
        "pca_df['GMM_Rank'] = pca_df['GMM_Anomaly_Score'].rank(ascending=False).astype(int)\n",
        "print(f\"GMM (n_components={n_components}) scoring complete.\")\n",
        "print(f\"\\nFinal ranked dataframe shape: {pca_df.shape}\")"
      ],
      "metadata": {
        "id": "lLHbBEVMVbXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Indider threat incident dates\n",
        "target_1 = ('PLJ1771', datetime(2010, 8, 12).date())\n",
        "target_2 = [\n",
        "    ('CDE1846', datetime(2011, 4, 21).date()),\n",
        "    ('CDE1846', datetime(2011, 4, 25).date()),\n",
        "]\n",
        "target_events = [target_1] + target_2\n",
        "TOTAL_EVENTS = pca_df.shape[0]\n",
        "\n",
        "filtered_results = pca_df.loc[target_events]\n",
        "\n",
        "target_output = filtered_results[['GMM_Anomaly_Score', 'GMM_Rank']].sort_values(by='GMM_Rank', ascending=True)\n",
        "\n",
        "target_output['GMM_Percentile_Rank'] = (target_output['GMM_Rank'] / TOTAL_EVENTS) * 100\n",
        "\n",
        "target_output['GMM_Percentile_Rank'] = target_output['GMM_Percentile_Rank'].round(4).astype(str) + '%'\n",
        "target_output['GMM_Anomaly_Score'] = target_output['GMM_Anomaly_Score'].round(4)\n",
        "\n",
        "print(f\"GMM Anomaly Ranks and Percentiles (based on N={TOTAL_EVENTS} total events):\")\n",
        "print(\"----------------\")\n",
        "target_output.to_csv(\"insider_event_ranks_targets_with_percentile.csv\")\n",
        "display(target_output)"
      ],
      "metadata": {
        "id": "10Ri-BVPYSnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 Anomaly Lookup\n",
        "top_20_output = pca_df.sort_values(by='GMM_Anomaly_Score', ascending=False).head(20)\n",
        "\n",
        "print(\"--- Top 20 Overall Highest GMM Anomaly Scores ---\")\n",
        "print(f\"Total Events Scored: {pca_df.shape[0]}\")\n",
        "\n",
        "# Display and save the Top 20 results\n",
        "top_20_output = top_20_output[['GMM_Anomaly_Score', 'GMM_Rank']]\n",
        "top_20_output.to_csv(\"top_20_gmm_anomalies.csv\")\n",
        "display(top_20_output)"
      ],
      "metadata": {
        "id": "aGgh5qz-0pYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphing Results"
      ],
      "metadata": {
        "id": "hPe79k7Ppjhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_anomaly_trend(user_id, start_date, end_date, anomaly_df):\n",
        "\n",
        "    # Filter by user and then by date range\n",
        "    user_df = anomaly_df.loc[user_id]\n",
        "    user_events = user_df.loc[start_date:end_date].copy()\n",
        "\n",
        "    if user_events.empty:\n",
        "        print(f\"No scored events found for {user_id} in the time period.\")\n",
        "        return\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(user_events.index, user_events['GMM_Anomaly_Score'], marker='o', linestyle='-', color='red', linewidth=2)\n",
        "\n",
        "    plt.title(f'GMM Anomaly Score Trend for User {user_id}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('GMM Anomaly Score (Higher is More Anomalous)')\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    file_name = f'{user_id}_anomaly_trend.png'\n",
        "    plt.savefig(file_name)\n",
        "    print(f\"Graph saved as {file_name}. You can see the file above.\")\n",
        "    print(\"Data points used for the plot:\")\n",
        "    print(user_events[['GMM_Anomaly_Score', 'GMM_Rank']])\n",
        "\n",
        "\n",
        "# Plot CDE1846\n",
        "plot_anomaly_trend(\n",
        "    user_id='CDE1846',\n",
        "    start_date=date(2010, 1, 2),\n",
        "    end_date=date(2011, 6, 1),\n",
        "    anomaly_df=pca_df\n",
        ")\n",
        "\n",
        "# Plot PLJ1771\n",
        "plot_anomaly_trend(\n",
        "    user_id='PLJ1771',\n",
        "    start_date=date(2010, 1, 2),\n",
        "    end_date=date(2011, 6, 1),\n",
        "    anomaly_df=pca_df\n",
        ")\n"
      ],
      "metadata": {
        "id": "zLBOOg0_dg5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}