{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KicChVC2OFsZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/235Kaggle/235kaggle'\n",
        "\n",
        "print(\"Files in data folder:\")\n",
        "print(os.listdir(data_folder))\n",
        "\n",
        "def read_csv(file_name):\n",
        "    \"\"\"Helper to read a CSV from the data folder.\"\"\"\n",
        "    return pd.read_csv(os.path.join(data_folder, file_name))\n",
        "\n",
        "logon      = read_csv('logon.csv')\n",
        "emails     = read_csv('email.csv')\n",
        "files      = read_csv('file.csv')\n",
        "decoys     = read_csv('decoy_file.csv')\n",
        "device     = read_csv('device.csv')\n",
        "psychometric = read_csv('psychometric.csv')\n",
        "insiders   = read_csv('insiders.csv')\n",
        "\n",
        "display(logon.head())\n",
        "display(device.head())\n",
        "display(emails.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logon_df  = logon.copy()\n",
        "device_df = device.copy()\n",
        "email_df  = emails.copy()\n",
        "\n",
        "logon_df['date']  = pd.to_datetime(logon_df['date'])\n",
        "device_df['date'] = pd.to_datetime(device_df['date'])\n",
        "email_df['date']  = pd.to_datetime(email_df['date'])\n",
        "\n",
        "for df in [logon_df, device_df, email_df]:\n",
        "    df['day'] = df['date'].dt.date\n",
        "    df['hour'] = df['date'].dt.hour\n",
        "\n",
        "    df['is_after_hours'] = (df['hour'] < 6) | (df['hour'] >= 18)\n",
        "\n",
        "    df['is_weekend'] = df['date'].dt.dayofweek >= 5\n",
        "\n",
        "print(\"Date parsing complete\")\n",
        "display(logon_df.head())\n",
        "display(device_df.head())\n",
        "display(email_df.head())\n"
      ],
      "metadata": {
        "id": "6KsbEkiySujL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "all_users = set(logon_df['user'].unique())\n",
        "print(f\"Total unique users: {len(all_users)}\")\n",
        "\n",
        "\n",
        "user_pc_counts = (\n",
        "    logon_df\n",
        "    .groupby(['user', 'pc'])\n",
        "    .size()\n",
        "    .reset_index(name='count')\n",
        ")\n",
        "\n",
        "\n",
        "user_primary_pc_df = user_pc_counts.loc[\n",
        "    user_pc_counts.groupby('user')['count'].idxmax()\n",
        "][['user', 'pc']]\n",
        "\n",
        "user_primary_pc = dict(zip(user_primary_pc_df['user'], user_primary_pc_df['pc']))\n",
        "\n",
        "print(\"User-PC mapping complete. Sample:\")\n",
        "list(user_primary_pc.items())[:10]\n"
      ],
      "metadata": {
        "id": "rK_DShoRTVrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_days = set()\n",
        "for df in [logon_df, device_df, email_df]:\n",
        "    user_days.update(zip(df['user'], df['day']))\n",
        "print(f\"Total user-day combinations: {len(user_days):,}\")\n",
        "\n",
        "\n",
        "logon_agg = (\n",
        "    logon_df[logon_df['activity'] == 'Logon']\n",
        "    .groupby(['user', 'day'])\n",
        "    .agg(\n",
        "        logon_count=('id', 'count'),\n",
        "        after_hours_logons=('is_after_hours', 'sum'),\n",
        "        unique_pcs=('pc', 'nunique'),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "\n",
        "logon_df['is_primary_pc'] = logon_df.apply(\n",
        "    lambda x: x['pc'] == user_primary_pc.get(x['user'], ''), axis=1\n",
        ")\n",
        "\n",
        "foreign_logons = (\n",
        "    logon_df[(logon_df['activity'] == 'Logon') & (~logon_df['is_primary_pc'])]\n",
        "    .groupby(['user', 'day'])\n",
        "    .size()\n",
        "    .reset_index(name='foreign_pc_logons')\n",
        ")\n",
        "\n",
        "\n",
        "device_agg = (\n",
        "    device_df[device_df['activity'] == 'Connect']\n",
        "    .groupby(['user', 'day'])\n",
        "    .agg(\n",
        "        device_connects=('id', 'count'),\n",
        "        after_hours_device=('is_after_hours', 'sum'),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "\n",
        "def count_recipients(row):\n",
        "    count = 0\n",
        "    for field in ['to', 'cc', 'bcc']:\n",
        "        if pd.notna(row[field]) and row[field]:\n",
        "            count += len(str(row[field]).split(';'))\n",
        "    return count\n",
        "\n",
        "email_df['recipient_count'] = email_df.apply(count_recipients, axis=1)\n",
        "\n",
        "email_sent = email_df[email_df['activity'] == 'Send']\n",
        "\n",
        "email_agg = (\n",
        "    email_sent\n",
        "    .groupby(['user', 'day'])\n",
        "    .agg(\n",
        "        emails_sent=('id', 'count'),\n",
        "        max_recipients=('recipient_count', 'max'),\n",
        "        total_recipients=('recipient_count', 'sum'),\n",
        "        avg_recipients=('recipient_count', 'mean'),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "\n",
        "features_df = logon_agg.copy()\n",
        "features_df = features_df.merge(foreign_logons, on=['user', 'day'], how='left')\n",
        "features_df = features_df.merge(device_agg,    on=['user', 'day'], how='left')\n",
        "features_df = features_df.merge(email_agg,     on=['user', 'day'], how='left')\n",
        "\n",
        "\n",
        "features_df = features_df.fillna(0)\n",
        "\n",
        "\n",
        "features_df['day'] = pd.to_datetime(features_df['day'])\n",
        "\n",
        "print(f\"Feature matrix shape: {features_df.shape}\")\n",
        "print(\"\\nFeatures:\")\n",
        "print(features_df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "display(features_df.head())\n"
      ],
      "metadata": {
        "id": "k_wTXGLITeFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: LOCAL OUTLIER FACTOR (LOF) MODEL\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import numpy as np\n",
        "\n",
        "feature_columns = [\n",
        "    'logon_count',\n",
        "    'after_hours_logons',\n",
        "    'unique_pcs',\n",
        "    'foreign_pc_logons',\n",
        "    'device_connects',\n",
        "    'after_hours_device',\n",
        "    'emails_sent',\n",
        "    'max_recipients',\n",
        "    'total_recipients',\n",
        "    'avg_recipients'\n",
        "]\n",
        "\n",
        "\n",
        "X = features_df[feature_columns].values\n",
        "print(f\"Total rows going into LOF: {X.shape[0]:,}\")\n",
        "\n",
        "lof_scaled = StandardScaler()\n",
        "X_scaled = lof_scaled.fit_transform(X)\n",
        "\n",
        "\n",
        "lof = LocalOutlierFactor(\n",
        "    n_neighbors=20,\n",
        "    contamination=0.02\n",
        ")\n",
        "\n",
        "labels_inLOF = lof.fit_predict(X_scaled)\n",
        "lof_score_raw = lof.negative_outlier_factor_\n",
        "\n",
        "features_df['lof_label'] = labels_inLOF\n",
        "features_df['lof_score_raw'] = lof_score_raw\n",
        "\n",
        "\n",
        "anomaly_strength = -lof_score_raw\n",
        "\n",
        "\n",
        "minimum_value = anomaly_strength.min()\n",
        "maximum_value = anomaly_strength.max()\n",
        "features_df['lof_score'] = (anomaly_strength - minimum_value) / (maximum_value - minimum_value + 1e-12)\n",
        "\n",
        "features_df['lof_percentile'] = pd.Series(anomaly_strength).rank(pct=True).values\n",
        "\n",
        "\n",
        "LOF_ANOMALIES = features_df[features_df['lof_label'] == -1]\n",
        "\n",
        "print(f\"\\nTotal records (all user-days): {len(features_df):,}\")\n",
        "print(f\"LOF anomalies detected: {len(LOF_ANOMALIES):,} \"\n",
        "      f\"({100*len(LOF_ANOMALIES)/len(features_df):.2f}%)\")\n",
        "\n",
        "print(\"\\nTop 20 most anomalous user-days (LOF):\")\n",
        "top_anomalies = features_df.nlargest(20, 'lof_score')[\n",
        "    ['user', 'day', 'lof_score', 'lof_score_raw'] + feature_columns\n",
        "]\n",
        "display(top_anomalies)\n",
        "\n"
      ],
      "metadata": {
        "id": "Rw40jULSTms6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scene3_user = features_df[\n",
        "    (features_df['user'] == 'PLJ1771') &\n",
        "    (features_df['day'] == '2010-08-12')\n",
        "]\n",
        "\n",
        "display(scene3_user)\n"
      ],
      "metadata": {
        "id": "GPLy7k3DCJUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene3_user = features_df[\n",
        "    (features_df['user'] == 'PLJ1771') &\n",
        "    (features_df['day'] == '2010-08-12')\n",
        "][['user', 'day', 'lof_score', 'lof_percentile']]\n",
        "\n",
        "display(scene3_user)\n"
      ],
      "metadata": {
        "id": "mFX8AbF6gMda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    scene4_user = (\n",
        "        insiders[insiders['scenario'] == 4]['user']\n",
        "        .dropna()\n",
        "        .unique()\n",
        "        .tolist()\n",
        "    )\n",
        "    print(\"Scenario 4 insider users:\", scene4_user)\n",
        "except Exception as e:\n",
        "    print(\"Could not load scenario 4 insiders:\", e)\n",
        "    scene4_user = []\n",
        "\n",
        "if len(scene4_user):\n",
        "\n",
        "\n",
        "    s4_rows = features_df[features_df['user'].isin(scene4_user)].copy()\n",
        "\n",
        "\n",
        "    s4_rows['day'] = pd.to_datetime(s4_rows['day'])\n",
        "\n",
        "\n",
        "    s4_rows = s4_rows.sort_values(['user', 'day'])\n",
        "\n",
        "\n",
        "    print(\"\\nScenario 4 insider user-days (LOF):\")\n",
        "    display(s4_rows[['user', 'day', 'lof_score', 'lof_percentile']])\n",
        "\n",
        "\n",
        "    print(\"\\nTop Scenario 4 anomalies (LOF):\")\n",
        "    s4_top = s4_rows.sort_values('lof_score', ascending=False).head(20)\n",
        "    display(s4_top[['user', 'day', 'lof_score', 'lof_percentile']])\n",
        "\n",
        "\n",
        "    suspicious = s4_rows[s4_rows['lof_percentile'] >= 0.99]\n",
        "    print(\"\\nScenario 4 days with LOF percentile ≥ 0.99:\")\n",
        "    if len(suspicious):\n",
        "        display(suspicious[['user', 'day', 'lof_score', 'lof_percentile']])\n",
        "    else:\n",
        "        print(\"No Scenario 4 insider days above 99th percentile.\")\n",
        "else:\n",
        "    print(\"Scenario 4 insiders not provided.\")\n"
      ],
      "metadata": {
        "id": "HKYquEmdVASz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plotting_lofAnomalies(features_df, user_id):\n",
        "\n",
        "\n",
        "    user_df = features_df[features_df[\"user\"] == user_id].copy()\n",
        "    user_df = user_df.sort_values(\"day\")\n",
        "\n",
        "\n",
        "    scores = user_df[\"lof_score\"].values.astype(float)\n",
        "\n",
        "\n",
        "    min_s, max_s = scores.min(), scores.max()\n",
        "    if max_s - min_s < 1e-12:\n",
        "\n",
        "        scaled = (scores - min_s) + 1e-6\n",
        "    else:\n",
        "        scaled = (scores - min_s) / (max_s - min_s)\n",
        "\n",
        "    user_df[\"scaled_score\"] = scaled\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(user_df[\"day\"], user_df[\"scaled_score\"], marker=\"o\", ms=4)\n",
        "    plt.title(f\"Anomaly scores over time — {user_id}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Scaled anomaly score (0–1 per user)\")\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plotting_lofAnomalies(features_df, \"PLJ1771\")\n",
        "plotting_lofAnomalies(features_df, \"CDE1846\")\n"
      ],
      "metadata": {
        "id": "KCmAr8oNbXiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def percentiles_plot(features_df, user_id):\n",
        "\n",
        "\n",
        "    df = features_df[features_df['user'] == user_id].copy()\n",
        "    df = df.sort_values(\"day\")\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"No data for {user_id}\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.plot(df['day'], df['lof_percentile'], marker='o', markersize=4, linewidth=1)\n",
        "    plt.title(f\"Anomaly Percentiles Over Time — {user_id}\", fontsize=15)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"LOF Percentile (0–1 scale)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "percentiles_plot(features_df, \"PLJ1771\")\n",
        "percentiles_plot(features_df, \"CDE1846\")\n"
      ],
      "metadata": {
        "id": "j6yrOG42em92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}